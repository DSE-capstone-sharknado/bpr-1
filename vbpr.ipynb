{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import lmdb\n",
    "import cPickle as pickle\n",
    "import numpy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16776\n",
      "11151\n"
     ]
    }
   ],
   "source": [
    "with open(\"man20k.pkl\", \"r\") as f:\n",
    "    user_id_mapping, item_id_mapping, train_ratings, test_ratings = pickle.load(f)\n",
    "\n",
    "image_features = {}\n",
    "db = lmdb.open('./features20k')\n",
    "with db.begin(write=False) as ctx:\n",
    "    for iid in item_id_mapping.values():\n",
    "        image_features[iid] = numpy.fromstring(ctx.get(str(iid)), dtype=numpy.float32)\n",
    "\n",
    "assert(len(item_id_mapping) == len(image_features))\n",
    "print len(user_id_mapping)\n",
    "print len(item_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniform_sample_batch(train_ratings, item_count, image_features, sample_count=20000, batch_size=5):\n",
    "    for i in range(sample_count):\n",
    "        t = []\n",
    "        iv = []\n",
    "        jv = []\n",
    "        for b in xrange(batch_size):\n",
    "            u = random.sample(train_ratings.keys(), 1)[0]\n",
    "            i = random.sample(train_ratings[u], 1)[0]\n",
    "            j = random.randint(0, item_count-1)\n",
    "            while j in train_ratings[u]:\n",
    "                j = random.randint(0, item_count-1)\n",
    "            t.append([u, i, j])\n",
    "            iv.append(image_features[i])\n",
    "            jv.append(image_features[j])\n",
    "        yield numpy.asarray(t), numpy.vstack(tuple(iv)), numpy.vstack(tuple(jv))\n",
    "\n",
    "def test_batch_generator_by_user(train_ratings, test_ratings, item_count, image_features):\n",
    "    # using leave one cv\n",
    "    for u in test_ratings.keys():\n",
    "        i = test_ratings[u]\n",
    "        t = []\n",
    "        ilist = []\n",
    "        jlist = []\n",
    "        for j in range(item_count):\n",
    "            if j != test_ratings[u] and not (j in train_ratings[u]):\n",
    "                # find item not in test[u] and train[u]\n",
    "                t.append([u, i, j])\n",
    "                ilist.append(image_features[i])\n",
    "                jlist.append(image_features[j])\n",
    "        yield numpy.asarray(t), numpy.vstack(tuple(ilist)), numpy.vstack(tuple(jlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vbpr(user_count, item_count, hidden_dim=20, hidden_img_dim=128, \n",
    "         learning_rate = 0.001,\n",
    "         l2_regulization = 0.01, \n",
    "         bias_regulization=1.0):\n",
    "    \"\"\"\n",
    "    user_count: total number of users\n",
    "    item_count: total number of items\n",
    "    hidden_dim: hidden feature size of MF\n",
    "    hidden_img_dim: [4096, hidden_img_dim]\n",
    "    \"\"\"\n",
    "    u = tf.placeholder(tf.int32, [None])\n",
    "    i = tf.placeholder(tf.int32, [None])\n",
    "    j = tf.placeholder(tf.int32, [None])\n",
    "    iv = tf.placeholder(tf.float32, [None, 4096])\n",
    "    jv = tf.placeholder(tf.float32, [None, 4096])\n",
    "    \n",
    "    with tf.device(\"/gpu:1\"):\n",
    "        user_emb_w = tf.get_variable(\"user_emb_w\", [user_count+1, hidden_dim], \n",
    "                                    initializer=tf.random_normal_initializer(0, 0.1))\n",
    "        user_img_w = tf.get_variable(\"user_img_w\", [user_count+1, hidden_img_dim],\n",
    "                                    initializer=tf.random_normal_initializer(0, 0.1))\n",
    "        item_emb_w = tf.get_variable(\"item_emb_w\", [item_count+1, hidden_dim], \n",
    "                                    initializer=tf.random_normal_initializer(0, 0.1))\n",
    "        item_b = tf.get_variable(\"item_b\", [item_count+1, 1], \n",
    "                                    initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        u_emb = tf.nn.embedding_lookup(user_emb_w, u)\n",
    "        u_img = tf.nn.embedding_lookup(user_img_w, u)\n",
    "        \n",
    "        i_emb = tf.nn.embedding_lookup(item_emb_w, i)\n",
    "        i_b = tf.nn.embedding_lookup(item_b, i)\n",
    "        j_emb = tf.nn.embedding_lookup(item_emb_w, j)\n",
    "        j_b = tf.nn.embedding_lookup(item_b, j)\n",
    "    \n",
    "    with tf.device(\"/gpu:1\"):\n",
    "        img_emb_w = tf.get_variable(\"image_embedding_weights\", [4096, hidden_img_dim], \n",
    "                                   initializer=tf.random_normal_initializer(0, 0.1))\n",
    "\n",
    "        img_i_j = tf.matmul(iv - jv,  img_emb_w)\n",
    "\n",
    "        # MF predict: u_i > u_j\n",
    "        x = i_b - j_b + tf.reduce_sum(tf.mul(u_emb, (i_emb - j_emb)), 1, keep_dims=True) + \\\n",
    "            tf.reduce_sum(tf.mul(u_img, img_i_j),1, keep_dims=True)\n",
    "\n",
    "        # auc score is used in test/cv\n",
    "        # reduce_mean is reasonable BECAUSE\n",
    "        # all test (i, j) pairs of one user is in ONE batch\n",
    "        auc = tf.reduce_mean(tf.to_float(x > 0))\n",
    "\n",
    "        l2_norm = tf.add_n([\n",
    "                tf.reduce_sum(tf.mul(u_emb, u_emb)), \n",
    "                tf.reduce_sum(tf.mul(u_img, u_img)),\n",
    "                tf.reduce_sum(tf.mul(i_emb, i_emb)),\n",
    "                tf.reduce_sum(tf.mul(j_emb, j_emb)),\n",
    "                tf.reduce_sum(tf.mul(img_emb_w, img_emb_w)),\n",
    "                bias_regulization * tf.reduce_sum(tf.mul(i_b, i_b)),\n",
    "                bias_regulization * tf.reduce_sum(tf.mul(j_b, j_b))\n",
    "            ])\n",
    "\n",
    "        loss = l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(x)))\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    return u, i, j, iv, jv, loss, auc, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1\n",
      "train_loss: 1.90607950926\n",
      "epoch  2\n",
      "train_loss: 0.847671033382\n",
      "epoch  3\n",
      "train_loss: 0.471264983296\n",
      "epoch  4\n",
      "train_loss: 0.28013004905\n",
      "epoch  5\n",
      "train_loss: 0.16987822926\n",
      "epoch  6\n",
      "train_loss: 0.10363506636\n",
      "epoch  7\n",
      "train_loss: 0.0634256706461\n",
      "epoch  8\n",
      "train_loss: 0.0388962389901\n",
      "epoch  9\n",
      "train_loss: 0.0238909451365\n",
      "test_loss:  0.0185006202516  auc:  0.528875898521\n",
      "\n",
      "epoch  10\n",
      "train_loss: 0.0147129922137\n",
      "epoch  11\n",
      "train_loss: 0.00908719892614\n",
      "epoch  12\n",
      "train_loss: 0.00563998709526\n",
      "epoch  13\n",
      "train_loss: 0.00352557315351\n",
      "epoch  14\n",
      "train_loss: 0.00222928830911\n",
      "epoch  15\n",
      "train_loss: 0.00143267278047\n",
      "epoch  16\n",
      "train_loss: 0.000944744668785\n",
      "epoch  17\n",
      "train_loss: 0.000645333021879\n",
      "epoch  18\n",
      "train_loss: 0.000461429908581\n",
      "epoch  19\n",
      "train_loss: 0.000348555094795\n",
      "test_loss:  0.000201286266862  auc:  0.696867971356\n",
      "\n",
      "epoch  20\n",
      "train_loss: 0.000279322677117\n",
      "epoch  21\n",
      "train_loss: 0.000236824362859\n",
      "epoch  22\n",
      "train_loss: 0.000210696356109\n",
      "epoch  23\n",
      "train_loss: 0.0001946867191\n",
      "epoch  24\n",
      "train_loss: 0.000184854761115\n",
      "epoch  25\n",
      "train_loss: 0.000178814592335\n",
      "epoch  26\n",
      "train_loss: 0.000175114339741\n",
      "epoch  27\n",
      "train_loss: 0.000172837277001\n",
      "epoch  28\n",
      "train_loss: 0.000171442347579\n",
      "epoch  29\n",
      "train_loss: 0.000170585585118\n",
      "test_loss:  6.32324662892e-05  auc:  0.724187425064\n",
      "\n",
      "epoch  30\n",
      "train_loss: 0.000170058965712\n",
      "epoch  31\n",
      "train_loss: 0.000169736123702\n",
      "epoch  32\n",
      "train_loss: 0.000169537788141\n",
      "epoch  33\n",
      "train_loss: 0.00016941606754\n",
      "epoch  34\n",
      "train_loss: 0.000169341344794\n",
      "epoch  35\n",
      "train_loss: 0.000169295391766\n",
      "epoch  36\n",
      "train_loss: 0.000169267251971\n",
      "epoch  37\n",
      "train_loss: 0.000169249952829\n",
      "epoch  38\n",
      "train_loss: 0.000169239334558\n",
      "epoch  39\n",
      "train_loss: 0.000169232823362\n",
      "test_loss:  6.21836653008e-05  auc:  0.724219947274\n",
      "\n",
      "epoch  40\n",
      "train_loss: 0.000169228824583\n",
      "epoch  41\n",
      "train_loss: 0.000169226372818\n",
      "epoch  42\n",
      "train_loss: 0.000169224862679\n",
      "epoch  43\n",
      "train_loss: 0.000169223932986\n",
      "epoch  44\n",
      "train_loss: 0.000169223372417\n",
      "epoch  45\n",
      "train_loss: 0.000169223024946\n",
      "epoch  46\n",
      "train_loss: 0.00016922280498\n",
      "epoch  47\n",
      "train_loss: 0.000169222674565\n",
      "epoch  48\n",
      "train_loss: 0.00016922258999\n",
      "epoch  49\n",
      "train_loss: 0.000169222545519\n",
      "test_loss:  6.21756902582e-05  auc:  0.723119083448\n",
      "\n",
      "epoch  50\n",
      "train_loss: 0.000169222513592\n",
      "epoch  51\n",
      "train_loss: 0.000169222495519\n",
      "epoch  52\n",
      "train_loss: 0.000169222475291\n",
      "epoch  53\n",
      "train_loss: 0.000169222473953\n",
      "epoch  54\n",
      "train_loss: 0.000169222472527\n",
      "epoch  55\n",
      "train_loss: 0.000169222468918\n",
      "epoch  56\n",
      "train_loss: 0.00016922246528\n",
      "epoch  57\n",
      "train_loss: 0.000169222464378\n",
      "epoch  58\n",
      "train_loss: 0.00016922246883\n",
      "epoch  59\n",
      "train_loss: 0.000169222466211\n",
      "test_loss:  6.2175628813e-05  auc:  0.722753278903\n",
      "\n",
      "epoch  60\n",
      "train_loss: 0.000169222461642\n",
      "epoch  61\n",
      "train_loss: 0.000169222460943\n",
      "epoch  62\n",
      "train_loss: 0.000169222467171\n",
      "epoch  63\n",
      "train_loss: 0.000169222463272\n",
      "epoch  64\n",
      "train_loss: 0.000169222463941\n",
      "epoch  65\n",
      "train_loss: 0.000169222463475\n",
      "epoch  66\n",
      "train_loss: 0.000169222464552\n",
      "epoch  67\n",
      "train_loss: 0.000169222464378\n",
      "epoch  68\n",
      "train_loss: 0.00016922246461\n",
      "epoch  69\n",
      "train_loss: 0.000169222466939\n",
      "test_loss:  6.2175629888e-05  auc:  0.722208490817\n",
      "\n",
      "epoch  70\n",
      "train_loss: 0.000169222467724\n",
      "epoch  71\n",
      "train_loss: 0.000169222470169\n",
      "epoch  72\n",
      "train_loss: 0.000169222464523\n",
      "epoch  73\n",
      "train_loss: 0.000169222463737\n",
      "epoch  74\n",
      "train_loss: 0.000169222465745\n",
      "epoch  75\n",
      "train_loss: 0.000169222462137\n",
      "epoch  76\n",
      "train_loss: 0.00016922245975\n",
      "epoch  77\n",
      "train_loss: 0.000169222466619\n",
      "epoch  78\n",
      "train_loss: 0.000169222466473\n",
      "epoch  79\n",
      "train_loss: 0.000169222463766\n",
      "test_loss:  6.21756297265e-05  auc:  0.722351304475\n",
      "\n",
      "epoch  80\n",
      "train_loss: 0.00016922246496\n",
      "epoch  81\n",
      "train_loss: 0.000169222462078\n",
      "epoch  82\n",
      "train_loss: 0.000169222465833\n",
      "epoch  83\n",
      "train_loss: 0.000169222463941\n",
      "epoch  84\n",
      "train_loss: 0.000169222466211\n",
      "epoch  85\n",
      "train_loss: 0.000169222466589\n",
      "epoch  86\n",
      "train_loss: 0.000169222464407\n",
      "epoch  87\n",
      "train_loss: 0.000169222467317\n",
      "epoch  88\n",
      "train_loss: 0.00016922246074\n",
      "epoch  89\n",
      "train_loss: 0.000169222460274\n",
      "test_loss:  6.21756326492e-05  auc:  0.722816082116\n",
      "\n",
      "epoch  90\n",
      "train_loss: 0.00016922246493\n",
      "epoch  91\n",
      "train_loss: 0.000169222466822\n",
      "epoch  92\n",
      "train_loss: 0.000169222465222\n",
      "epoch  93\n",
      "train_loss: 0.000169222465833\n",
      "epoch  94\n",
      "train_loss: 0.000169222465833\n",
      "epoch  95\n",
      "train_loss: 0.000169222463446\n",
      "epoch  96\n",
      "train_loss: 0.000169222459896\n",
      "epoch  97\n",
      "train_loss: 0.000169222458673\n",
      "epoch  98\n",
      "train_loss: 0.000169222463417\n",
      "epoch  99\n",
      "train_loss: 0.000169222469209\n",
      "test_loss:  6.21756314117e-05  auc:  0.722785097113\n",
      "\n",
      "epoch  100\n",
      "train_loss: 0.000169222462137\n",
      "epoch  101\n",
      "train_loss: 0.000169222466182\n",
      "epoch  102\n",
      "train_loss: 0.000169222465629\n",
      "epoch  103\n",
      "train_loss: 0.000169222466764\n",
      "epoch  104\n",
      "train_loss: 0.000169222461729\n",
      "epoch  105\n",
      "train_loss: 0.000169222460157\n",
      "epoch  106\n",
      "train_loss: 0.000169222466444\n",
      "epoch  107\n",
      "train_loss: 0.000169222462078\n",
      "epoch  108\n",
      "train_loss: 0.000169222469965\n",
      "epoch  109\n",
      "train_loss: 0.000169222461467\n",
      "test_loss:  6.21756326817e-05  auc:  0.722071970966\n",
      "\n",
      "epoch  110\n",
      "train_loss: 0.000169222466735\n",
      "epoch  111\n",
      "train_loss: 0.000169222465716\n",
      "epoch  112\n",
      "train_loss: 0.000169222462282\n",
      "epoch  113\n",
      "train_loss: 0.0001692224656\n",
      "epoch  114\n",
      "train_loss: 0.000169222462515\n",
      "epoch  115\n",
      "train_loss: 0.000169222461438\n",
      "epoch  116\n",
      "train_loss: 0.000169222465367\n",
      "epoch  117\n",
      "train_loss: 0.000169222462253\n",
      "epoch  118\n",
      "train_loss: 0.000169222469121\n",
      "epoch  119\n",
      "train_loss: 0.000169222467724\n",
      "test_loss:  6.21756272506e-05  auc:  0.722286653913\n",
      "\n",
      "epoch  120\n",
      "train_loss: 0.000169222470693\n",
      "epoch  121\n",
      "train_loss: 0.000169222465454\n",
      "epoch  122\n",
      "train_loss: 0.000169222464174\n",
      "epoch  123\n",
      "train_loss: 0.00016922246333\n",
      "epoch  124\n",
      "train_loss: 0.000169222468161\n",
      "epoch  125\n",
      "train_loss: 0.000169222466124\n",
      "epoch  126\n",
      "train_loss: 0.000169222470693\n",
      "epoch  127\n",
      "train_loss: 0.00016922246106\n",
      "epoch  128\n",
      "train_loss: 0.000169222462835\n",
      "epoch  129\n",
      "train_loss: 0.000169222466298\n",
      "test_loss:  6.21756290916e-05  auc:  0.722704050756\n",
      "\n",
      "epoch  130\n",
      "train_loss: 0.000169222466124\n",
      "epoch  131\n",
      "train_loss: 0.000169222466706\n",
      "epoch  132\n",
      "train_loss: 0.000169222464872\n",
      "epoch  133\n",
      "train_loss: 0.000169222464494\n",
      "epoch  134\n",
      "train_loss: 0.000169222464494\n",
      "epoch  135\n",
      "train_loss: 0.000169222461845\n",
      "epoch  136\n",
      "train_loss: 0.000169222460914\n",
      "epoch  137\n",
      "train_loss: 0.000169222462224\n",
      "epoch  138\n",
      "train_loss: 0.000169222462748\n",
      "epoch  139\n",
      "train_loss: 0.000169222463446\n",
      "test_loss:  6.21756309968e-05  auc:  0.72228236522\n",
      "\n",
      "epoch  140\n",
      "train_loss: 0.000169222461205\n",
      "epoch  141\n",
      "train_loss: 0.000169222464698\n",
      "epoch  142\n",
      "train_loss: 0.000169222460303\n",
      "epoch  143\n",
      "train_loss: 0.000169222465804\n",
      "epoch  144\n",
      "train_loss: 0.000169222465425\n",
      "epoch  145\n",
      "train_loss: 0.000169222464669\n",
      "epoch  146\n",
      "train_loss: 0.000169222463883\n",
      "epoch  147\n",
      "train_loss: 0.000169222465745\n",
      "epoch  148\n",
      "train_loss: 0.000169222465076\n",
      "epoch  149\n",
      "train_loss: 0.000169222464436\n",
      "test_loss:  6.2175629206e-05  auc:  0.722723938225\n",
      "\n",
      "epoch  150\n",
      "train_loss: 0.000169222467928\n",
      "epoch  151\n",
      "train_loss: 0.000169222460099\n",
      "epoch  152\n",
      "train_loss: 0.000169222463417\n",
      "epoch  153\n",
      "train_loss: 0.000169222461438\n",
      "epoch  154\n",
      "train_loss: 0.000169222467579\n",
      "epoch  155\n",
      "train_loss: 0.000169222467812\n",
      "epoch  156\n",
      "train_loss: 0.000169222467259\n",
      "epoch  157\n",
      "train_loss: 0.000169222467812\n",
      "epoch  158\n",
      "train_loss: 0.000169222459401\n",
      "epoch  159\n",
      "train_loss: 0.00016922246461\n",
      "test_loss:  6.21756271442e-05  auc:  0.722820235079\n",
      "\n",
      "epoch  160\n",
      "train_loss: 0.000169222467724\n",
      "epoch  161\n",
      "train_loss: 0.0001692224617\n",
      "epoch  162\n",
      "train_loss: 0.000169222457102\n",
      "epoch  163\n",
      "train_loss: 0.000169222466968\n",
      "epoch  164\n",
      "train_loss: 0.000169222464639\n",
      "epoch  165\n",
      "train_loss: 0.000169222467201\n",
      "epoch  166\n",
      "train_loss: 0.000169222469238\n",
      "epoch  167\n",
      "train_loss: 0.000169222464901\n",
      "epoch  168\n",
      "train_loss: 0.000169222463155\n",
      "epoch  169\n",
      "train_loss: 0.000169222461845\n",
      "test_loss:  6.21756316478e-05  auc:  0.722910937715\n",
      "\n",
      "epoch  170\n",
      "train_loss: 0.000169222463592\n",
      "epoch  171\n",
      "train_loss: 0.000169222463854\n",
      "epoch  172\n",
      "train_loss: 0.000169222467375\n",
      "epoch  173\n",
      "train_loss: 0.000169222461554\n",
      "epoch  174\n",
      "train_loss: 0.000169222461176\n",
      "epoch  175\n",
      "train_loss: 0.000169222468481\n",
      "epoch  176\n",
      "train_loss: 0.000169222465134\n",
      "epoch  177\n",
      "train_loss: 0.000169222468452\n",
      "epoch  178\n",
      "train_loss: 0.000169222461787\n",
      "epoch  179\n",
      "train_loss: 0.00016922246429\n",
      "test_loss:  6.21756275126e-05  auc:  0.721525889624\n",
      "\n",
      "epoch  180\n",
      "train_loss: 0.000169222471101\n",
      "epoch  181\n",
      "train_loss: 0.000169222465862\n",
      "epoch  182\n",
      "train_loss: 0.000169222465105\n",
      "epoch  183\n",
      "train_loss: 0.000169222463621\n",
      "epoch  184\n",
      "train_loss: 0.000169222462748\n",
      "epoch  185\n",
      "train_loss: 0.000169222466502\n",
      "epoch  186\n",
      "train_loss: 0.000169222464348\n",
      "epoch  187\n",
      "train_loss: 0.00016922246496\n",
      "epoch  188\n",
      "train_loss: 0.00016922247046\n",
      "epoch  189\n",
      "train_loss: 0.000169222464639\n",
      "test_loss:  6.21756274816e-05  auc:  0.721473329031\n",
      "\n",
      "epoch  190\n",
      "train_loss: 0.000169222466735\n",
      "epoch  191\n",
      "train_loss: 0.000169222467754\n",
      "epoch  192\n",
      "train_loss: 0.000169222463475\n",
      "epoch  193\n",
      "train_loss: 0.000169222464028\n",
      "epoch  194\n",
      "train_loss: 0.000169222461118\n",
      "epoch  195\n",
      "train_loss: 0.000169222463068\n",
      "epoch  196\n",
      "train_loss: 0.000169222463766\n",
      "epoch  197\n",
      "train_loss: 0.00016922246234\n",
      "epoch  198\n",
      "train_loss: 0.000169222458586\n",
      "epoch  199\n",
      "train_loss: 0.00016922246365\n",
      "test_loss:  6.21756310719e-05  auc:  0.721355034757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_count = len(user_id_mapping)\n",
    "item_count = len(item_id_mapping)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    with tf.variable_scope('vbpr'):\n",
    "        u, i, j, iv, jv, loss, auc, train_op = vbpr(user_count, item_count)\n",
    "    \n",
    "    session.run(tf.initialize_all_variables())\n",
    "    \n",
    "    for epoch in range(1, 200):\n",
    "        print \"epoch \", epoch\n",
    "        _loss_train = 0.0\n",
    "        sample_count = 500\n",
    "        batch_size = 4096\n",
    "        for d, _iv, _jv in uniform_sample_batch(train_ratings, item_count, image_features,\n",
    "                                                batch_size=batch_size, sample_count=sample_count):\n",
    "            _loss, _ = session.run([loss, train_op], feed_dict={\n",
    "                    u:d[:,0], i:d[:,1], j:d[:,2], iv:_iv, jv:_jv\n",
    "                })\n",
    "            _loss_train += _loss\n",
    "        print \"train_loss:\", _loss_train/sample_count\n",
    "        \n",
    "        if epoch % 10 != 0:\n",
    "            continue\n",
    "        \n",
    "        _auc_all = 0\n",
    "        _loss_test = 0.0\n",
    "        _test_user_count = len(test_ratings)\n",
    "        for d, _iv, _jv in test_batch_generator_by_user(train_ratings, \n",
    "                                                      test_ratings, item_count, image_features):\n",
    "            _loss, _auc = session.run([loss, auc], feed_dict={\n",
    "                    u:d[:,0], i:d[:,1], j:d[:,2], iv:_iv, jv:_jv\n",
    "                })\n",
    "            _loss_test += _loss\n",
    "            _auc_all += _auc\n",
    "        print \"test_loss: \", _loss_test/_test_user_count, \" auc: \", _auc_all/_test_user_count\n",
    "        print \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
